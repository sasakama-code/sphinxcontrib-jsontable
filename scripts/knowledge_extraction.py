#!/usr/bin/env python3
"""
KPTåˆ†æã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡ºãƒ»ç®¡ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    python scripts/knowledge_extraction.py extract --kpt-file plan/analysis/kpt_analysis.md
    python scripts/knowledge_extraction.py validate --knowledge-dir knowledge/
    python scripts/knowledge_extraction.py review --knowledge-file knowledge/technical_architecture/rag_pipeline.md
"""

import argparse
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import hashlib

class KnowledgeExtractionRules:
    """KPTãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡ºãƒ«ãƒ¼ãƒ«"""
    
    def __init__(self):
        self.extraction_criteria = {
            "technical_knowledge": {
                "minimum_success_rate": 0.80,
                "replication_potential": "high",
                "documentation_completeness": "comprehensive",
                "code_examples_available": True
            },
            "strategic_knowledge": {
                "measurable_impact": True,
                "decision_rationale_clear": True,
                "stakeholder_alignment": "achieved",
                "timeline_efficiency": 1.50  # 150%ä»¥ä¸Š
            },
            "process_knowledge": {
                "workflow_documentation": "complete",
                "quality_metrics": "defined",
                "automation_potential": "high",
                "scalability_proven": True
            }
        }
        
        self.knowledge_categories = {
            "technical_architecture": {
                "path": "knowledge/technical_architecture/",
                "description": "æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ»è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³",
                "examples": ["ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ", "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹æˆ", "APIè¨­è¨ˆ"]
            },
            "development_patterns": {
                "path": "knowledge/development_patterns/",
                "description": "é–‹ç™ºæ‰‹æ³•ãƒ»ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "examples": ["ãƒ†ã‚¹ãƒˆæˆ¦ç•¥", "å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³", "å“è³ªä¿è¨¼"]
            },
            "business_insights": {
                "path": "knowledge/business_insights/",
                "description": "ãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿãƒ»å¸‚å ´åˆ†æ",
                "examples": ["å¸‚å ´æˆ¦ç•¥", "ç«¶åˆåˆ†æ", "ROIåˆ†æ"]
            },
            "project_management": {
                "path": "knowledge/project_management/",
                "description": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ãƒ»æ„æ€æ±ºå®š",
                "examples": ["è¨ˆç”»æ‰‹æ³•", "ãƒªã‚¹ã‚¯ç®¡ç†", "ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ç®¡ç†"]
            },
            "lessons_learned": {
                "path": "knowledge/lessons_learned/",
                "description": "æ•™è¨“ãƒ»æ”¹å–„ç‚¹",
                "examples": ["æˆåŠŸè¦å› ", "å¤±æ•—åˆ†æ", "æ”¹å–„ææ¡ˆ"]
            }
        }
        
        self.security_levels = {
            "public": {
                "description": "å…¬é–‹å¯èƒ½",
                "gitignore": False,
                "examples": ["ä¸€èˆ¬çš„æŠ€è¡“ãƒ‘ã‚¿ãƒ¼ãƒ³", "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹è²¢çŒ®"]
            },
            "internal": {
                "description": "å†…éƒ¨é™å®š",
                "gitignore": True,
                "examples": ["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰å®Ÿè£…", "çµ„ç¹”å†…å­¦ç¿’"]
            },
            "confidential": {
                "description": "æ©Ÿå¯†",
                "gitignore": True,
                "examples": ["æˆ¦ç•¥åˆ†æ", "ç«¶åˆæƒ…å ±", "åç›Šãƒ‡ãƒ¼ã‚¿"]
            },
            "restricted": {
                "description": "åˆ¶é™",
                "gitignore": True,
                "examples": ["å€‹äººæƒ…å ±", "æœªç™ºè¡¨æƒ…å ±", "ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼æ©Ÿå¯†"]
            }
        }

class KPTAnalyzer:
    """KPTåˆ†æãƒ•ã‚¡ã‚¤ãƒ«è§£æ"""
    
    def __init__(self):
        self.kpt_patterns = {
            "keep_section": r"##\s*ğŸŸ¢\s*\*\*Keep[^#]*?(?=##|\Z)",
            "problem_section": r"##\s*ğŸ”´\s*\*\*Problem[^#]*?(?=##|\Z)",
            "try_section": r"##\s*ğŸš€\s*\*\*Try[^#]*?(?=##|\Z)",
            "success_metrics": r"([0-9]+(?:\.[0-9]+)?%|[0-9]+(?:\.[0-9]+)?å€|[0-9]+(?:\.[0-9]+)?x)",
            "implementation_details": r"```(?:python|javascript|bash|sql)(.*?)```",
            "quantitative_results": r"([0-9,]+(?:\.[0-9]+)?)\s*(è¡Œ|ä»¶|ç§’|ãƒ¶æœˆ|%)"
        }
    
    def parse_kpt_file(self, kpt_file_path: str) -> Dict[str, Any]:
        """KPTãƒ•ã‚¡ã‚¤ãƒ«è§£æ"""
        
        with open(kpt_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        analysis_result = {
            "file_path": kpt_file_path,
            "extraction_date": datetime.now().isoformat(),
            "sections": {},
            "success_metrics": [],
            "implementation_details": [],
            "quantitative_results": []
        }
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
        for section_name, pattern in self.kpt_patterns.items():
            if section_name.endswith('_section'):
                matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
                if matches:
                    analysis_result["sections"][section_name] = matches[0]
        
        # æˆåŠŸæŒ‡æ¨™æŠ½å‡º
        success_metrics = re.findall(self.kpt_patterns["success_metrics"], content)
        analysis_result["success_metrics"] = success_metrics
        
        # å®Ÿè£…è©³ç´°æŠ½å‡º
        implementation_details = re.findall(self.kpt_patterns["implementation_details"], content, re.DOTALL)
        analysis_result["implementation_details"] = implementation_details
        
        # å®šé‡çš„çµæœæŠ½å‡º
        quantitative_results = re.findall(self.kpt_patterns["quantitative_results"], content)
        analysis_result["quantitative_results"] = quantitative_results
        
        return analysis_result
    
    def extract_knowledge_candidates(self, analysis_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒŠãƒ¬ãƒƒã‚¸å€™è£œæŠ½å‡º"""
        
        candidates = []
        
        # Keepé …ç›®ã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡º
        if "keep_section" in analysis_result["sections"]:
            keep_content = analysis_result["sections"]["keep_section"]
            keep_items = self._parse_keep_items(keep_content)
            
            for item in keep_items:
                if self._meets_extraction_criteria(item):
                    candidate = {
                        "type": "success_pattern",
                        "source_section": "keep",
                        "title": item.get("title", "Unknown"),
                        "content": item.get("content", ""),
                        "success_evidence": item.get("evidence", []),
                        "replication_potential": self._assess_replication_potential(item),
                        "suggested_category": self._suggest_knowledge_category(item),
                        "security_level": self._assess_security_level(item)
                    }
                    candidates.append(candidate)
        
        # Tryé …ç›®ã‹ã‚‰æ”¹å–„ææ¡ˆæŠ½å‡º
        if "try_section" in analysis_result["sections"]:
            try_content = analysis_result["sections"]["try_section"]
            try_items = self._parse_try_items(try_content)
            
            for item in try_items:
                candidate = {
                    "type": "improvement_opportunity",
                    "source_section": "try",
                    "title": item.get("title", "Unknown"),
                    "content": item.get("content", ""),
                    "expected_impact": item.get("impact", ""),
                    "implementation_approach": item.get("approach", ""),
                    "suggested_category": self._suggest_knowledge_category(item),
                    "security_level": self._assess_security_level(item)
                }
                candidates.append(candidate)
        
        return candidates
    
    def _parse_keep_items(self, keep_content: str) -> List[Dict[str, Any]]:
        """Keepé …ç›®è§£æ"""
        
        # é …ç›®åˆ†å‰²ï¼ˆ### ã§å§‹ã¾ã‚‹é …ç›®ï¼‰
        items = re.split(r'\n###\s*\*?\*?([^#\n]+)', keep_content)
        
        parsed_items = []
        for i in range(1, len(items), 2):
            if i + 1 < len(items):
                title = items[i].strip()
                content = items[i + 1].strip()
                
                # æˆåŠŸè¨¼æ‹ æŠ½å‡º
                evidence = re.findall(r'\*\*å®Ÿç¸¾\*\*:(.*?)(?=\*\*|$)', content, re.DOTALL)
                
                parsed_items.append({
                    "title": title,
                    "content": content,
                    "evidence": [e.strip() for e in evidence]
                })
        
        return parsed_items
    
    def _parse_try_items(self, try_content: str) -> List[Dict[str, Any]]:
        """Tryé …ç›®è§£æ"""
        
        items = re.split(r'\n###\s*\*?\*?([^#\n]+)', try_content)
        
        parsed_items = []
        for i in range(1, len(items), 2):
            if i + 1 < len(items):
                title = items[i].strip()
                content = items[i + 1].strip()
                
                # æœŸå¾…åŠ¹æœæŠ½å‡º
                impact = re.findall(r'\*\*æœŸå¾…åŠ¹æœ\*\*:(.*?)(?=\*\*|$)', content, re.DOTALL)
                approach = re.findall(r'\*\*ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\*\*:(.*?)(?=\*\*|$)', content, re.DOTALL)
                
                parsed_items.append({
                    "title": title,
                    "content": content,
                    "impact": impact[0].strip() if impact else "",
                    "approach": approach[0].strip() if approach else ""
                })
        
        return parsed_items
    
    def _meets_extraction_criteria(self, item: Dict[str, Any]) -> bool:
        """æŠ½å‡ºåŸºæº–ãƒã‚§ãƒƒã‚¯"""
        
        # æˆåŠŸè¨¼æ‹ ã®æœ‰ç„¡
        if not item.get("evidence"):
            return False
        
        # å…·ä½“çš„ãªå®Ÿè£…è©³ç´°ã®æœ‰ç„¡
        content = item.get("content", "")
        if not re.search(r'```|å®Ÿè£…|ã‚³ãƒ¼ãƒ‰|æ‰‹æ³•', content):
            return False
        
        # å®šé‡çš„æŒ‡æ¨™ã®æœ‰ç„¡
        if not re.search(r'[0-9]+(?:\.[0-9]+)?[%å€x]', content):
            return False
        
        return True
    
    def _assess_replication_potential(self, item: Dict[str, Any]) -> str:
        """å†ç¾å¯èƒ½æ€§è©•ä¾¡"""
        
        content = item.get("content", "")
        
        # å…·ä½“çš„ãªæ‰‹é †ãƒ»ã‚³ãƒ¼ãƒ‰ã®æœ‰ç„¡
        has_concrete_steps = bool(re.search(r'```|æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—|å®Ÿè£…', content))
        
        # å‰ææ¡ä»¶ã®æ˜è¨˜
        has_prerequisites = bool(re.search(r'å‰æ|æ¡ä»¶|è¦ä»¶', content))
        
        # æˆåŠŸæŒ‡æ¨™ã®å®šç¾©
        has_metrics = bool(re.search(r'æŒ‡æ¨™|ãƒ¡ãƒˆãƒªã‚¯ã‚¹|æ¸¬å®š', content))
        
        score = sum([has_concrete_steps, has_prerequisites, has_metrics])
        
        if score >= 3:
            return "high"
        elif score >= 2:
            return "medium"
        else:
            return "low"
    
    def _suggest_knowledge_category(self, item: Dict[str, Any]) -> str:
        """ãƒŠãƒ¬ãƒƒã‚¸ã‚«ãƒ†ã‚´ãƒªæ¨å®š"""
        
        content = item.get("content", "") + " " + item.get("title", "")
        content_lower = content.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        category_keywords = {
            "technical_architecture": ["ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£", "è¨­è¨ˆ", "API", "ã‚·ã‚¹ãƒ†ãƒ ", "çµ±åˆ", "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"],
            "development_patterns": ["ãƒ†ã‚¹ãƒˆ", "å®Ÿè£…", "é–‹ç™º", "ã‚³ãƒ¼ãƒ‰", "å“è³ª", "ãƒ‘ã‚¿ãƒ¼ãƒ³"],
            "business_insights": ["å¸‚å ´", "ç«¶åˆ", "æˆ¦ç•¥", "ROI", "åç›Š", "ãƒ“ã‚¸ãƒã‚¹"],
            "project_management": ["è¨ˆç”»", "æ„æ€æ±ºå®š", "ç®¡ç†", "ãƒªã‚¹ã‚¯", "ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼"],
            "lessons_learned": ["æ•™è¨“", "æ”¹å–„", "å¤±æ•—", "æˆåŠŸè¦å› ", "å­¦ç¿’"]
        }
        
        scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            scores[category] = score
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
        return max(scores.items(), key=lambda x: x[1])[0]
    
    def _assess_security_level(self, item: Dict[str, Any]) -> str:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        
        content = item.get("content", "") + " " + item.get("title", "")
        content_lower = content.lower()
        
        # æ©Ÿå¯†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        restricted_keywords = ["å€‹äººæƒ…å ±", "æ©Ÿå¯†", "ç§˜å¯†", "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", "api key"]
        confidential_keywords = ["æˆ¦ç•¥", "ç«¶åˆ", "åç›Š", "roi", "å¸‚å ´åˆ†æ", "ç‰¹è¨±"]
        internal_keywords = ["å†…éƒ¨", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰", "çµ„ç¹”", "ãƒãƒ¼ãƒ "]
        
        if any(keyword in content_lower for keyword in restricted_keywords):
            return "restricted"
        elif any(keyword in content_lower for keyword in confidential_keywords):
            return "confidential"
        elif any(keyword in content_lower for keyword in internal_keywords):
            return "internal"
        else:
            return "public"

class KnowledgeDocumentGenerator:
    """ãƒŠãƒ¬ãƒƒã‚¸æ–‡æ›¸ç”Ÿæˆ"""
    
    def __init__(self):
        self.document_template = """# ğŸ¯ {title}

**æŠ½å‡ºå…ƒ**: KPTåˆ†æ ({source_file})  
**æ¤œè¨¼æœŸé–“**: {validation_period}  
**å®Ÿè¨¼åŠ¹æœ**: {proven_effects}

---

## ğŸ“‹ æ¦‚è¦

{overview}

## ğŸ¯ æ ¸å¿ƒåŸå‰‡

{core_principles}

## ğŸ”§ å®Ÿè£…è©³ç´°

{implementation_details}

## ğŸ“Š æˆåŠŸæŒ‡æ¨™

{success_metrics}

## ğŸ¯ é©ç”¨æ¡ä»¶

{application_conditions}

## ğŸ”„ ç¶™ç¶šçš„æ”¹å–„

{continuous_improvement}

---

## ğŸ† çµè«–

{conclusion}
"""
    
    def generate_knowledge_document(
        self, 
        candidate: Dict[str, Any], 
        kpt_analysis: Dict[str, Any]
    ) -> str:
        """ãƒŠãƒ¬ãƒƒã‚¸æ–‡æ›¸ç”Ÿæˆ"""
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã®æº–å‚™
        template_vars = {
            "title": candidate.get("title", "Unknown Knowledge"),
            "source_file": os.path.basename(kpt_analysis.get("file_path", "unknown")),
            "validation_period": self._extract_validation_period(kpt_analysis),
            "proven_effects": self._extract_proven_effects(candidate),
            "overview": self._generate_overview(candidate),
            "core_principles": self._extract_core_principles(candidate),
            "implementation_details": self._extract_implementation_details(candidate),
            "success_metrics": self._extract_success_metrics(candidate),
            "application_conditions": self._generate_application_conditions(candidate),
            "continuous_improvement": self._generate_improvement_section(candidate),
            "conclusion": self._generate_conclusion(candidate)
        }
        
        return self.document_template.format(**template_vars)
    
    def _extract_validation_period(self, kpt_analysis: Dict[str, Any]) -> str:
        """æ¤œè¨¼æœŸé–“æŠ½å‡º"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜æŠ½å‡ºã‚’è©¦è¡Œ
        file_path = kpt_analysis.get("file_path", "")
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', file_path)
        
        if date_match:
            return date_match.group(1)
        else:
            return datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    def _extract_proven_effects(self, candidate: Dict[str, Any]) -> str:
        """å®Ÿè¨¼åŠ¹æœæŠ½å‡º"""
        
        content = candidate.get("content", "")
        
        # å®šé‡çš„åŠ¹æœã‚’æ¤œç´¢
        effects = re.findall(r'([0-9]+(?:\.[0-9]+)?[%å€x][^ã€‚]*)', content)
        
        if effects:
            return "ã€".join(effects[:3])  # æœ€å¤§3ã¤ã®åŠ¹æœ
        else:
            return "å®šæ€§çš„æ”¹å–„åŠ¹æœç¢ºèªæ¸ˆã¿"
    
    def _generate_overview(self, candidate: Dict[str, Any]) -> str:
        """æ¦‚è¦ç”Ÿæˆ"""
        
        content = candidate.get("content", "")
        
        # æœ€åˆã®æ®µè½ã‚’æ¦‚è¦ã¨ã—ã¦ä½¿ç”¨
        paragraphs = content.split('\n\n')
        overview = paragraphs[0] if paragraphs else content[:200]
        
        return overview.strip()
    
    def _extract_core_principles(self, candidate: Dict[str, Any]) -> str:
        """æ ¸å¿ƒåŸå‰‡æŠ½å‡º"""
        
        content = candidate.get("content", "")
        
        # åŸå‰‡ãƒ»ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«é–¢ã™ã‚‹è¨˜è¿°ã‚’æ¤œç´¢
        principles = []
        
        # ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚’æ¤œç´¢
        numbered_items = re.findall(r'\d+\.\s*\*\*([^*]+)\*\*[^0-9]*', content)
        principles.extend(numbered_items)
        
        # ç®‡æ¡æ›¸ãã‚’æ¤œç´¢
        bullet_items = re.findall(r'-\s*\*\*([^*]+)\*\*', content)
        principles.extend(bullet_items)
        
        if principles:
            formatted_principles = []
            for i, principle in enumerate(principles[:5], 1):
                formatted_principles.append(f"### {i}. **{principle.strip()}**")
            return "\n\n".join(formatted_principles)
        else:
            return "### 1. **å®Ÿè¨¼æ¸ˆã¿æ‰‹æ³•ã®é©ç”¨**\n- æˆåŠŸäº‹ä¾‹ã«åŸºã¥ãç¢ºå®Ÿãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
    
    def _extract_implementation_details(self, candidate: Dict[str, Any]) -> str:
        """å®Ÿè£…è©³ç´°æŠ½å‡º"""
        
        content = candidate.get("content", "")
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œç´¢
        code_blocks = re.findall(r'```(\w+)?\n(.*?)```', content, re.DOTALL)
        
        if code_blocks:
            formatted_code = []
            for lang, code in code_blocks:
                lang = lang or "python"
                formatted_code.append(f"```{lang}\n{code.strip()}\n```")
            return "\n\n".join(formatted_code)
        else:
            # å®Ÿè£…ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’æ¤œç´¢
            impl_patterns = [
                r'å®Ÿè£…[^ã€‚]*ã€‚',
                r'æ‰‹æ³•[^ã€‚]*ã€‚',
                r'ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ[^ã€‚]*ã€‚'
            ]
            
            implementations = []
            for pattern in impl_patterns:
                matches = re.findall(pattern, content)
                implementations.extend(matches)
            
            return "\n".join(implementations) if implementations else "å…·ä½“çš„ãªå®Ÿè£…æ‰‹é †ã¯å…ƒã®KPTåˆ†æã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
    
    def _extract_success_metrics(self, candidate: Dict[str, Any]) -> str:
        """æˆåŠŸæŒ‡æ¨™æŠ½å‡º"""
        
        content = candidate.get("content", "")
        
        # æˆåŠŸæŒ‡æ¨™ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¤œç´¢
        metrics = re.findall(r'([0-9]+(?:\.[0-9]+)?[%å€x][^ã€‚]*)', content)
        
        if metrics:
            formatted_metrics = []
            for metric in metrics:
                formatted_metrics.append(f"- {metric}")
            return "\n".join(formatted_metrics)
        else:
            return "- å“è³ªç›®æ¨™ã®é”æˆ\n- ãƒ—ãƒ­ã‚»ã‚¹åŠ¹ç‡ã®æ”¹å–„\n- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼æº€è¶³åº¦å‘ä¸Š"
    
    def _generate_application_conditions(self, candidate: Dict[str, Any]) -> str:
        """é©ç”¨æ¡ä»¶ç”Ÿæˆ"""
        
        content = candidate.get("content", "")
        
        # å‰ææ¡ä»¶ãƒ»åˆ¶ç´„ã‚’æ¤œç´¢
        conditions = []
        
        condition_patterns = [
            r'å‰æ[^ã€‚]*ã€‚',
            r'æ¡ä»¶[^ã€‚]*ã€‚',
            r'è¦ä»¶[^ã€‚]*ã€‚',
            r'åˆ¶ç´„[^ã€‚]*ã€‚'
        ]
        
        for pattern in condition_patterns:
            matches = re.findall(pattern, content)
            conditions.extend(matches)
        
        if conditions:
            formatted_conditions = []
            for condition in conditions:
                formatted_conditions.append(f"- {condition}")
            return "\n".join(formatted_conditions)
        else:
            return "- é¡ä¼¼ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç’°å¢ƒã§ã®é©ç”¨\n- ååˆ†ãªãƒªã‚½ãƒ¼ã‚¹ãƒ»æ™‚é–“ã®ç¢ºä¿\n- ãƒãƒ¼ãƒ ã®æŠ€è¡“ãƒ¬ãƒ™ãƒ«ãƒ»å­¦ç¿’æ„æ¬²"
    
    def _generate_improvement_section(self, candidate: Dict[str, Any]) -> str:
        """æ”¹å–„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        
        return """### å­¦ç¿’ãƒ«ãƒ¼ãƒ—
- å®Ÿè£… â†’ æ¸¬å®š â†’ åˆ†æ â†’ æ”¹å–„ â†’ æ¬¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé©ç”¨

### ãƒŠãƒ¬ãƒƒã‚¸è“„ç©
- æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¨™æº–åŒ–
- å¤±æ•—äº‹ä¾‹ã®åˆ†æã¨å¯¾ç­–
- æ‰‹æ³•ã®ä»–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®é©ç”¨"""
    
    def _generate_conclusion(self, candidate: Dict[str, Any]) -> str:
        """çµè«–ç”Ÿæˆ"""
        
        title = candidate.get("title", "ã“ã®æ‰‹æ³•")
        
        return f"**{title}ã«ã‚ˆã‚Šã€å®Ÿè¨¼æ¸ˆã¿ã®é«˜ã„åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚**\n\n### æ ¸å¿ƒçš„ä¾¡å€¤\n1. **åŠ¹ç‡æ€§**: å®Ÿè£…ãƒ»é‹ç”¨åŠ¹ç‡ã®å¤§å¹…å‘ä¸Š\n2. **å“è³ª**: ç¢ºå®Ÿãªå“è³ªç›®æ¨™é”æˆ\n3. **å†ç¾æ€§**: ä»–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®é©ç”¨å¯èƒ½æ€§\n4. **å­¦ç¿’**: çµ„ç¹”èƒ½åŠ›ã®ç¶™ç¶šçš„å‘ä¸Š\n\nã“ã®æ‰‹æ³•ã¯ã€é¡ä¼¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãŠã‘ã‚‹æ–°ã—ã„ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã¨ãªã‚Šã¾ã™ã€‚ ğŸš€"

class KnowledgeValidator:
    """ãƒŠãƒ¬ãƒƒã‚¸å“è³ªæ¤œè¨¼"""
    
    def __init__(self):
        self.quality_criteria = {
            "content_quality": {
                "has_empirical_evidence": 0.25,
                "includes_concrete_examples": 0.25,
                "defines_success_metrics": 0.25,
                "specifies_constraints": 0.25
            },
            "documentation_quality": {
                "follows_standard_format": 0.30,
                "has_searchable_keywords": 0.25,
                "includes_code_examples": 0.25,
                "proper_categorization": 0.20
            },
            "practical_utility": {
                "replication_feasibility": 0.40,
                "clear_implementation_steps": 0.30,
                "troubleshooting_info": 0.30
            },
            "maintainability": {
                "version_info": 0.25,
                "update_schedule": 0.25,
                "related_documents": 0.25,
                "review_history": 0.25
            }
        }
    
    def validate_knowledge_document(self, file_path: str) -> Dict[str, Any]:
        """ãƒŠãƒ¬ãƒƒã‚¸æ–‡æ›¸æ¤œè¨¼"""
        
        if not os.path.exists(file_path):
            return {"error": "File not found", "file_path": file_path}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        validation_results = {}
        overall_score = 0.0
        
        for dimension, criteria in self.quality_criteria.items():
            dimension_score = 0.0
            dimension_details = {}
            
            for criterion, weight in criteria.items():
                score = self._evaluate_criterion(content, criterion)
                dimension_details[criterion] = score
                dimension_score += score * weight
            
            validation_results[dimension] = {
                "score": dimension_score,
                "details": dimension_details
            }
            overall_score += dimension_score
        
        overall_score /= len(self.quality_criteria)
        
        return {
            "file_path": file_path,
            "overall_quality_score": overall_score,
            "validation_results": validation_results,
            "approval_status": "approved" if overall_score >= 0.8 else "revision_required",
            "recommendations": self._generate_recommendations(validation_results)
        }
    
    def _evaluate_criterion(self, content: str, criterion: str) -> float:
        """åŸºæº–è©•ä¾¡"""
        
        evaluation_map = {
            "has_empirical_evidence": lambda c: 1.0 if re.search(r'å®Ÿè¨¼|å®Ÿç¸¾|ãƒ‡ãƒ¼ã‚¿|çµæœ|æˆæœ', c) else 0.0,
            "includes_concrete_examples": lambda c: 1.0 if re.search(r'```|ä¾‹:|å®Ÿè£…|ã‚³ãƒ¼ãƒ‰', c) else 0.0,
            "defines_success_metrics": lambda c: 1.0 if re.search(r'æŒ‡æ¨™|ãƒ¡ãƒˆãƒªã‚¯ã‚¹|æ¸¬å®š|[0-9]+%', c) else 0.0,
            "specifies_constraints": lambda c: 1.0 if re.search(r'å‰æ|æ¡ä»¶|åˆ¶ç´„|è¦ä»¶', c) else 0.0,
            "follows_standard_format": lambda c: 1.0 if re.search(r'# ğŸ¯.*\*\*æŠ½å‡ºå…ƒ\*\*.*## ğŸ“‹ æ¦‚è¦', c, re.DOTALL) else 0.0,
            "has_searchable_keywords": lambda c: 1.0 if re.search(r'ğŸ¯|ğŸ“‹|ğŸ”§|ğŸ“Š', c) else 0.0,
            "includes_code_examples": lambda c: min(1.0, len(re.findall(r'```', c)) / 4),
            "proper_categorization": lambda c: 1.0 if re.search(r'technical_|development_|business_|project_|lessons_', c) else 0.0,
            "replication_feasibility": lambda c: 1.0 if re.search(r'æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—|å®Ÿè£…|æ–¹æ³•', c) else 0.0,
            "clear_implementation_steps": lambda c: min(1.0, len(re.findall(r'\d+\.|###|\*\*æ‰‹é †', c)) / 5),
            "troubleshooting_info": lambda c: 1.0 if re.search(r'æ³¨æ„|å•é¡Œ|ã‚¨ãƒ©ãƒ¼|å¯¾å¿œ', c) else 0.0,
            "version_info": lambda c: 1.0 if re.search(r'\d{4}-\d{2}-\d{2}|\d{4}å¹´\d{1,2}æœˆ', c) else 0.0,
            "update_schedule": lambda c: 1.0 if re.search(r'æ›´æ–°|è¦‹ç›´ã—|ãƒ¬ãƒ“ãƒ¥ãƒ¼', c) else 0.0,
            "related_documents": lambda c: 1.0 if re.search(r'é–¢é€£|å‚ç…§|å‚è€ƒ', c) else 0.0,
            "review_history": lambda c: 1.0 if re.search(r'å±¥æ­´|ãƒãƒ¼ã‚¸ãƒ§ãƒ³|å¤‰æ›´', c) else 0.0
        }
        
        evaluator = evaluation_map.get(criterion, lambda c: 0.5)
        return evaluator(content)
    
    def _generate_recommendations(self, validation_results: Dict[str, Any]) -> List[str]:
        """æ”¹å–„æ¨å¥¨ç”Ÿæˆ"""
        
        recommendations = []
        
        for dimension, results in validation_results.items():
            if results["score"] < 0.7:
                low_criteria = [
                    criterion for criterion, score in results["details"].items()
                    if score < 0.5
                ]
                
                if low_criteria:
                    recommendations.append(f"{dimension}ã®æ”¹å–„ãŒå¿…è¦: {', '.join(low_criteria)}")
        
        return recommendations

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    parser = argparse.ArgumentParser(description="KPTåˆ†æãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡ºãƒ»ç®¡ç†ãƒ„ãƒ¼ãƒ«")
    subparsers = parser.add_subparsers(dest="command", help="åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰")
    
    # extract ã‚³ãƒãƒ³ãƒ‰
    extract_parser = subparsers.add_parser("extract", help="KPTåˆ†æã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡º")
    extract_parser.add_argument("--kpt-file", required=True, help="KPTåˆ†æãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    extract_parser.add_argument("--output-dir", default="knowledge/", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    extract_parser.add_argument("--auto-generate", action="store_true", help="è‡ªå‹•æ–‡æ›¸ç”Ÿæˆ")
    
    # validate ã‚³ãƒãƒ³ãƒ‰
    validate_parser = subparsers.add_parser("validate", help="ãƒŠãƒ¬ãƒƒã‚¸æ–‡æ›¸æ¤œè¨¼")
    validate_parser.add_argument("--knowledge-dir", default="knowledge/", help="ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    validate_parser.add_argument("--knowledge-file", help="ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼")
    
    # review ã‚³ãƒãƒ³ãƒ‰
    review_parser = subparsers.add_parser("review", help="ãƒŠãƒ¬ãƒƒã‚¸å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼")
    review_parser.add_argument("--knowledge-file", required=True, help="ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«")
    review_parser.add_argument("--output-report", help="ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ")
    
    args = parser.parse_args()
    
    if args.command == "extract":
        extract_knowledge(args)
    elif args.command == "validate":
        validate_knowledge(args)
    elif args.command == "review":
        review_knowledge(args)
    else:
        parser.print_help()

def extract_knowledge(args):
    """ãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡ºå®Ÿè¡Œ"""
    
    print(f"ğŸ” KPTåˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {args.kpt_file}")
    
    # KPTåˆ†æãƒ•ã‚¡ã‚¤ãƒ«è§£æ
    analyzer = KPTAnalyzer()
    analysis_result = analyzer.parse_kpt_file(args.kpt_file)
    
    # ãƒŠãƒ¬ãƒƒã‚¸å€™è£œæŠ½å‡º
    candidates = analyzer.extract_knowledge_candidates(analysis_result)
    
    print(f"ğŸ“‹ {len(candidates)}å€‹ã®ãƒŠãƒ¬ãƒƒã‚¸å€™è£œã‚’æŠ½å‡º")
    
    # å€™è£œä¸€è¦§è¡¨ç¤º
    for i, candidate in enumerate(candidates, 1):
        print(f"{i}. {candidate['title']} ({candidate['suggested_category']}) - {candidate['security_level']}")
    
    if args.auto_generate:
        # è‡ªå‹•æ–‡æ›¸ç”Ÿæˆ
        generator = KnowledgeDocumentGenerator()
        
        for candidate in candidates:
            if candidate['security_level'] in ['internal', 'confidential']:
                # ãƒŠãƒ¬ãƒƒã‚¸æ–‡æ›¸ç”Ÿæˆ
                document_content = generator.generate_knowledge_document(candidate, analysis_result)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                category = candidate['suggested_category']
                title_safe = re.sub(r'[^\w\-_]', '_', candidate['title'])
                file_name = f"{title_safe}.md"
                
                output_path = Path(args.output_dir) / category / file_name
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(document_content)
                
                print(f"âœ… ãƒŠãƒ¬ãƒƒã‚¸æ–‡æ›¸ç”Ÿæˆ: {output_path}")
    
    print("ğŸ¯ ãƒŠãƒ¬ãƒƒã‚¸æŠ½å‡ºå®Œäº†")

def validate_knowledge(args):
    """ãƒŠãƒ¬ãƒƒã‚¸æ¤œè¨¼å®Ÿè¡Œ"""
    
    validator = KnowledgeValidator()
    
    if args.knowledge_file:
        # ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        result = validator.validate_knowledge_document(args.knowledge_file)
        print(f"ğŸ“‹ æ¤œè¨¼çµæœ: {args.knowledge_file}")
        print(f"ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {result['overall_quality_score']:.2f}")
        print(f"æ‰¿èªçŠ¶æ³: {result['approval_status']}")
        
        if result['recommendations']:
            print("æ¨å¥¨æ”¹å–„ç‚¹:")
            for rec in result['recommendations']:
                print(f"  - {rec}")
    
    else:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“æ¤œè¨¼
        knowledge_dir = Path(args.knowledge_dir)
        
        if not knowledge_dir.exists():
            print(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {knowledge_dir}")
            return
        
        md_files = list(knowledge_dir.rglob("*.md"))
        
        print(f"ğŸ” {len(md_files)}å€‹ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ä¸­")
        
        total_score = 0.0
        approved_count = 0
        
        for md_file in md_files:
            result = validator.validate_knowledge_document(str(md_file))
            
            if 'error' not in result:
                total_score += result['overall_quality_score']
                if result['approval_status'] == 'approved':
                    approved_count += 1
                
                print(f"  {md_file.name}: {result['overall_quality_score']:.2f} ({result['approval_status']})")
        
        if md_files:
            avg_score = total_score / len(md_files)
            approval_rate = approved_count / len(md_files) * 100
            
            print(f"\nğŸ“Š æ¤œè¨¼ã‚µãƒãƒªãƒ¼:")
            print(f"  å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_score:.2f}")
            print(f"  æ‰¿èªç‡: {approval_rate:.1f}%")

def review_knowledge(args):
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Ÿè¡Œ"""
    
    validator = KnowledgeValidator()
    result = validator.validate_knowledge_document(args.knowledge_file)
    
    print(f"ğŸ“‹ è©³ç´°ãƒ¬ãƒ“ãƒ¥ãƒ¼: {args.knowledge_file}")
    print("=" * 50)
    
    if 'error' in result:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return
    
    print(f"ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {result['overall_quality_score']:.2f}")
    print(f"æ‰¿èªçŠ¶æ³: {result['approval_status']}")
    print()
    
    for dimension, dim_result in result['validation_results'].items():
        print(f"## {dimension}")
        print(f"ã‚¹ã‚³ã‚¢: {dim_result['score']:.2f}")
        
        for criterion, score in dim_result['details'].items():
            status = "âœ…" if score > 0.7 else "âš ï¸" if score > 0.3 else "âŒ"
            print(f"  {status} {criterion}: {score:.2f}")
        print()
    
    if result['recommendations']:
        print("## æ”¹å–„æ¨å¥¨")
        for rec in result['recommendations']:
            print(f"  ğŸ”§ {rec}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    if args.output_report:
        with open(args.output_report, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {args.output_report}")

if __name__ == "__main__":
    main()